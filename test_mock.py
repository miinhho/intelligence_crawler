"""
ëª¨ì˜ HTTP ì„œë²„ë¥¼ ì‚¬ìš©í•œ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸
ë¡œì»¬ì—ì„œ ë¹ ë¥´ê²Œ ë™ì‘ì„ ê²€ì¦
"""

import asyncio
from aiohttp import web
import json

# ëª¨ì˜ HTML í˜ì´ì§€ë“¤
MOCK_PAGES = {
    "/": """
    <html>
        <head><title>AI Research Hub</title></head>
        <body>
            <h1>Artificial Intelligence Research</h1>
            <p>Welcome to our AI research hub. We cover machine learning, deep learning, 
            and neural networks. This is the main page about artificial intelligence.</p>
            <nav>
                <a href="/machine-learning">Machine Learning Guide</a>
                <a href="/deep-learning">Deep Learning Tutorial</a>
                <a href="/neural-networks">Neural Networks Basics</a>
                <a href="/unrelated">Cooking Recipes</a>
            </nav>
        </body>
    </html>
    """,
    "/machine-learning": """
    <html>
        <head><title>Machine Learning Guide</title></head>
        <body>
            <h1>Machine Learning</h1>
            <p>Machine learning is a subset of artificial intelligence that enables systems 
            to learn from data. Popular algorithms include decision trees, random forests, 
            and support vector machines.</p>
            <a href="/supervised-learning">Supervised Learning</a>
            <a href="/unsupervised-learning">Unsupervised Learning</a>
            <a href="/">Back to Home</a>
        </body>
    </html>
    """,
    "/deep-learning": """
    <html>
        <head><title>Deep Learning Tutorial</title></head>
        <body>
            <h1>Deep Learning</h1>
            <p>Deep learning uses neural networks with multiple layers to process data. 
            It powers image recognition, natural language processing, and many AI applications. 
            Popular frameworks include TensorFlow and PyTorch.</p>
            <a href="/cnn">Convolutional Neural Networks</a>
            <a href="/rnn">Recurrent Neural Networks</a>
            <a href="/">Back to Home</a>
        </body>
    </html>
    """,
    "/neural-networks": """
    <html>
        <head><title>Neural Networks Basics</title></head>
        <body>
            <h1>Neural Networks</h1>
            <p>Neural networks are computing systems inspired by biological neural networks. 
            They consist of neurons, weights, and activation functions. Training involves 
            backpropagation and gradient descent.</p>
            <a href="/perceptron">The Perceptron</a>
            <a href="/">Back to Home</a>
        </body>
    </html>
    """,
    "/unrelated": """
    <html>
        <head><title>Cooking Recipes</title></head>
        <body>
            <h1>Delicious Recipes</h1>
            <p>Try our amazing pasta carbonara recipe. You'll need eggs, bacon, 
            parmesan cheese, and spaghetti. Cook the pasta al dente and mix with 
            the creamy egg sauce.</p>
            <a href="/desserts">Dessert Recipes</a>
            <a href="/">Back to Home</a>
        </body>
    </html>
    """,
    "/supervised-learning": """
    <html>
        <head><title>Supervised Learning</title></head>
        <body>
            <h1>Supervised Learning</h1>
            <p>In supervised learning, models learn from labeled training data. 
            Common tasks include classification and regression. Examples: spam detection, 
            price prediction.</p>
        </body>
    </html>
    """,
}


async def handle_request(request):
    """ëª¨ì˜ HTTP ìš”ì²­ í•¸ë“¤ëŸ¬"""
    path = request.path

    # robots.txt ì²˜ë¦¬
    if path == "/robots.txt":
        return web.Response(text="User-agent: *\nAllow: /\n", content_type="text/plain")

    # í˜ì´ì§€ ë°˜í™˜
    if path in MOCK_PAGES:
        return web.Response(text=MOCK_PAGES[path], content_type="text/html")

    # 404
    return web.Response(text="Not Found", status=404)


async def run_mock_server(port=8888):
    """ëª¨ì˜ ì„œë²„ ì‹¤í–‰"""
    app = web.Application()
    app.router.add_get("/{path:.*}", handle_request)

    runner = web.AppRunner(app)
    await runner.setup()
    site = web.TCPSite(runner, "localhost", port)
    await site.start()

    print(f"ğŸŒ Mock server running at http://localhost:{port}")
    return runner


async def test_with_mock_server():
    """ëª¨ì˜ ì„œë²„ë¡œ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸"""
    from crawler import IntelligentCrawler
    from crawler.crawler_engine import CrawlConfig

    print("=" * 70)
    print("Mock Server Crawl Test")
    print("=" * 70)

    # ëª¨ì˜ ì„œë²„ ì‹œì‘
    print("\n[1/5] Starting mock server...")
    server_runner = await run_mock_server(8888)

    try:
        # ì„¤ì •
        config = CrawlConfig(
            max_pages=6,  # 6ê°œ í˜ì´ì§€
            max_depth=2,  # ê¹Šì´ 2
            request_delay=0.1,  # ë¡œì»¬ì´ë¯€ë¡œ ë¹ ë¥´ê²Œ
            respect_robots_txt=True,
        )

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        print("\n[2/5] Initializing components...")
        crawler = IntelligentCrawler(config, enable_profiling=True)
        print("  âœ“ All components initialized")

        # í¬ë¡¤ë§ ì‹œì‘
        seed_url = "http://localhost:8888/"
        topic = "artificial intelligence machine learning"

        print("\n[3/5] Starting crawl...")
        print(f"  Seed URL: {seed_url}")
        print(f"  Topic: {topic}")
        print(f"  Max pages: {config.max_pages}")
        print(f"  Max depth: {config.max_depth}")
        print()

        results = await crawler.crawl(
            seed_urls=[seed_url],
            topic=topic,
        )

        print("\n[4/5] Crawl completed!")

        # ê²°ê³¼ ë¶„ì„
        stats = results["statistics"]
        print("\n  ğŸ“Š Statistics:")
        print(f"    Pages crawled: {stats['num_pages']}")
        print(f"    Links found: {stats['num_links']}")
        print(f"    Internal links: {stats.get('internal_links', 0)}")
        print(f"    External links: {stats.get('external_links', 0)}")
        print(f"    Avg relevance: {stats.get('avg_topic_relevance', 0):.3f}")
        print(f"    Graph density: {stats.get('density', 0):.3f}")

        # í˜ì´ì§€ë³„ ê´€ë ¨ë„
        print("\n  ğŸ“„ Pages by relevance:")
        for i, page in enumerate(results["pages"], 1):
            relevance_bar = "â–ˆ" * int(page["topic_relevance"] * 20)
            print(f"    {i}. [{page['topic_relevance']:.3f}] {relevance_bar}")
            print(f"       {page['title']}")
            print(f"       Depth: {page['depth']}, PageRank: {page['pagerank']:.4f}")

        # ë§í¬ ê´€ê³„
        print("\n  ğŸ”— Link relationships:")
        for i, link in enumerate(results["links"][:10], 1):
            print(f"    {i}. [{link['relevance']:.3f}] {link['anchor_text'][:40]}")
            print(
                f"       {link['source'].split('/')[-1]} â†’ {link['target'].split('/')[-1]}"
            )

        # ê²€ì¦
        print("\n[5/5] Validation:")

        # ê´€ë ¨ í˜ì´ì§€ê°€ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ì•˜ëŠ”ì§€ í™•ì¸
        ai_pages = [
            p
            for p in results["pages"]
            if any(
                word in p["title"].lower()
                for word in ["ai", "machine", "learning", "neural", "deep"]
            )
        ]
        unrelated = [
            p
            for p in results["pages"]
            if "cooking" in p["title"].lower() or "recipe" in p["title"].lower()
        ]

        if ai_pages:
            avg_ai_relevance = sum(p["topic_relevance"] for p in ai_pages) / len(
                ai_pages
            )
            print(f"  âœ“ AI-related pages average relevance: {avg_ai_relevance:.3f}")

        if unrelated:
            avg_unrelated_relevance = sum(
                p["topic_relevance"] for p in unrelated
            ) / len(unrelated)
            print(
                f"  âœ“ Unrelated pages average relevance: {avg_unrelated_relevance:.3f}"
            )

            if avg_unrelated_relevance < 0.5:
                print("  âœ“ Good! Unrelated content has low relevance")
            else:
                print("  âš  Warning: Unrelated content has high relevance")

        # ê²°ê³¼ ì €ì¥
        output_data = {
            "topic": topic,
            "seed_urls": [seed_url],
            "statistics": stats,
            "pages": results["pages"],
            "links": results["links"],
        }

        # Performance ë°ì´í„° ì¶”ê°€
        if "performance" in results:
            output_data["performance"] = results["performance"]
            print("\n  âš¡ Performance Summary:")
            perf = results["performance"]
            print(f"    Total time: {perf['total_time']:.2f}s")
            print(f"    Memory usage: {perf['current_memory_mb']:.2f}MB")
            print(f"    CPU usage: {perf['cpu_percent']:.1f}%")

        with open("mock_test_results.json", "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        print("\n  ğŸ’¾ Results saved to: mock_test_results.json")

        print("\n" + "=" * 70)
        print("âœ“ Mock server test completed successfully!")
        print("=" * 70)

    except Exception as e:
        print(f"\nâœ— Error during test: {e}")
        import traceback

        traceback.print_exc()

    finally:
        # ì„œë²„ ì¢…ë£Œ
        print("\nğŸ›‘ Shutting down mock server...")
        await server_runner.cleanup()


if __name__ == "__main__":
    asyncio.run(test_with_mock_server())
