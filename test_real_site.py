"""
ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ë¡œ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸
ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ í¬í•¨
"""

import asyncio
import json
from datetime import datetime

from crawler import IntelligentCrawler
from crawler.crawler_engine import CrawlConfig


async def test_real_website():
    """ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸"""
    print("=" * 70)
    print("Real Website Crawl Test with Profiling")
    print("=" * 70)

    # í…ŒìŠ¤íŠ¸ ì„¤ì •
    TEST_CONFIGS = [
        {
            "name": "Wikipedia AI",
            "seed_urls": ["https://en.wikipedia.org/wiki/Artificial_intelligence"],
            "topic": "artificial intelligence machine learning neural networks",
            "max_pages": 15,
            "max_depth": 2,
        },
        {
            "name": "Python Docs",
            "seed_urls": ["https://docs.python.org/3/tutorial/"],
            "topic": "python programming tutorial",
            "max_pages": 10,
            "max_depth": 1,
        },
        {
            "name": "ArXiv ML",
            "seed_urls": ["https://arxiv.org/list/cs.LG/recent"],
            "topic": "machine learning deep learning research",
            "max_pages": 10,
            "max_depth": 1,
        },
    ]

    # ì‚¬ìš©ìì—ê²Œ ì„ íƒ ìš”ì²­
    print("\nAvailable test sites:")
    for i, config in enumerate(TEST_CONFIGS, 1):
        print(f"  {i}. {config['name']}")
        print(f"     URL: {config['seed_urls'][0]}")
        print(f"     Topic: {config['topic']}")
        print(f"     Max pages: {config['max_pages']}")

    print("\n  0. Custom (enter your own)")

    try:
        choice = int(input("\nSelect test (0-3): "))
    except (ValueError, EOFError):
        choice = 1
        print(f"Using default: {TEST_CONFIGS[0]['name']}")

    # ì„¤ì • ì„ íƒ
    if choice == 0:
        test_config = {
            "name": "Custom",
            "seed_urls": [input("Enter seed URL: ")],
            "topic": input("Enter topic: "),
            "max_pages": int(input("Max pages (default 10): ") or "10"),
            "max_depth": int(input("Max depth (default 2): ") or "2"),
        }
    elif 1 <= choice <= len(TEST_CONFIGS):
        test_config = TEST_CONFIGS[choice - 1]
    else:
        print("Invalid choice, using default")
        test_config = TEST_CONFIGS[0]

    print(f"\n{'=' * 70}")
    print(f"Testing: {test_config['name']}")
    print(f"{'=' * 70}\n")

    # í¬ë¡¤ëŸ¬ ì„¤ì •
    config = CrawlConfig(
        max_pages=test_config["max_pages"],
        max_depth=test_config["max_depth"],
        request_delay=1.0,  # ì‹¤ì œ ì‚¬ì´íŠ¸ì´ë¯€ë¡œ ì˜ˆì˜ìƒ 1ì´ˆ
        max_concurrent_requests=3,  # ì„œë²„ ë¶€ë‹´ì„ ì¤„ì´ê¸° ìœ„í•´ 3ê°œë¡œ ì œí•œ
        respect_robots_txt=True,
    )

    print("[1/4] Initializing crawler with profiling...")
    crawler = IntelligentCrawler(config, enable_profiling=True)
    print("  âœ“ Crawler initialized\n")

    print("[2/4] Starting crawl...")
    print(f"  Seed URL: {test_config['seed_urls'][0]}")
    print(f"  Topic: {test_config['topic']}")
    print(f"  Max pages: {config.max_pages}")
    print(f"  Max depth: {config.max_depth}")
    print(f"  Request delay: {config.request_delay}s")
    print()

    start_time = datetime.now()

    try:
        results = await crawler.crawl(
            seed_urls=test_config["seed_urls"],
            topic=test_config["topic"],
            relevance_threshold=0.5,
        )

        end_time = datetime.now()
        elapsed = (end_time - start_time).total_seconds()

        print(f"\n[3/4] Crawl completed in {elapsed:.1f}s!")

        # ê²°ê³¼ ë¶„ì„
        stats = results["statistics"]
        print("\n  ğŸ“Š Crawl Statistics:")
        print(f"    Pages crawled: {stats['num_pages']}")
        print(f"    Links found: {stats['num_links']}")
        print(f"    Internal links: {stats.get('internal_links', 0)}")
        print(f"    External links: {stats.get('external_links', 0)}")
        print(f"    Avg relevance: {stats.get('avg_topic_relevance', 0):.3f}")
        print(f"    Graph density: {stats.get('density', 0):.3f}")

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        if "performance" in results:
            print("\n  âš¡ Performance Metrics:")
            perf = results["performance"]
            print(f"    Total time: {perf['total_time']:.2f}s")
            print(f"    Memory usage: {perf['current_memory_mb']:.2f}MB")
            print(f"    CPU usage: {perf['cpu_percent']:.1f}%")

            # ì‘ì—…ë³„ ì‹œê°„
            if perf.get("timings"):
                print("\n  â±ï¸  Operation Timings:")
                for op, timing in perf["timings"].items():
                    print(
                        f"    {op}: {timing['mean']:.3f}s avg "
                        f"(total: {timing['total']:.2f}s, count: {timing['count']})"
                    )

            # ìºì‹œ í†µê³„
            if perf.get("cache_stats"):
                print("\n  ğŸ’¾ Cache Statistics:")
                for component, cache_stats in perf["cache_stats"].items():
                    print(f"    {component}:")
                    for key, value in cache_stats.items():
                        print(f"      {key}: {value}")

        # ìƒìœ„ ê´€ë ¨ í˜ì´ì§€
        print("\n  ğŸ“„ Top Pages by Relevance:")
        sorted_pages = sorted(
            results["pages"], key=lambda x: x["topic_relevance"], reverse=True
        )
        for i, page in enumerate(sorted_pages[:5], 1):
            relevance_bar = "â–ˆ" * int(page["topic_relevance"] * 20)
            print(f"    {i}. [{page['topic_relevance']:.3f}] {relevance_bar}")
            print(f"       {page['title'][:60]}")
            print(f"       Depth: {page['depth']}, PageRank: {page['pagerank']:.4f}")

        # ê²°ê³¼ ì €ì¥
        print("\n[4/4] Saving results...")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"crawl_results_{test_config['name'].replace(' ', '_')}_{timestamp}.json"

        output_data = {
            "test_name": test_config["name"],
            "timestamp": datetime.now().isoformat(),
            "config": {
                "seed_urls": test_config["seed_urls"],
                "topic": test_config["topic"],
                "max_pages": config.max_pages,
                "max_depth": config.max_depth,
            },
            "statistics": stats,
            "pages": results["pages"],
            "links": results["links"][:100],  # ë§í¬ê°€ ë„ˆë¬´ ë§ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìƒìœ„ 100ê°œë§Œ
        }

        if "performance" in results:
            output_data["performance"] = results["performance"]

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        print(f"  ğŸ’¾ Results saved to: {filename}")

        # ìš”ì•½
        print(f"\n{'=' * 70}")
        print("âœ“ Test completed successfully!")
        print(f"{'=' * 70}")
        print(f"\nTotal pages: {stats['num_pages']}/{config.max_pages}")
        print(f"Avg relevance: {stats.get('avg_topic_relevance', 0):.3f}")
        print(f"Time: {elapsed:.1f}s")

        if "performance" in results:
            perf = results["performance"]
            print(f"Memory: {perf['current_memory_mb']:.2f}MB")
            print(f"CPU: {perf['cpu_percent']:.1f}%")

    except KeyboardInterrupt:
        print("\n\nâš ï¸  Crawl interrupted by user")
    except Exception as e:
        print(f"\nâœ— Error during crawl: {e}")
        import traceback

        traceback.print_exc()


async def compare_with_without_profiling():
    """í”„ë¡œíŒŒì¼ë§ ì˜¤ë²„í—¤ë“œ ì¸¡ì •"""
    print("\n" + "=" * 70)
    print("Profiling Overhead Test")
    print("=" * 70)

    seed_url = "https://example.com"
    topic = "example"

    config = CrawlConfig(max_pages=5, max_depth=1, request_delay=0.5)

    # Without profiling
    print("\n[1/2] Running without profiling...")
    crawler1 = IntelligentCrawler(config, enable_profiling=False)
    start1 = datetime.now()
    await crawler1.crawl(seed_urls=[seed_url], topic=topic)
    time1 = (datetime.now() - start1).total_seconds()
    print(f"  Time: {time1:.2f}s")

    # With profiling
    print("\n[2/2] Running with profiling...")
    crawler2 = IntelligentCrawler(config, enable_profiling=True)
    start2 = datetime.now()
    await crawler2.crawl(seed_urls=[seed_url], topic=topic)
    time2 = (datetime.now() - start2).total_seconds()
    print(f"  Time: {time2:.2f}s")

    # Overhead
    overhead = ((time2 - time1) / time1) * 100
    print(f"\n  Profiling overhead: {overhead:.1f}%")


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "--overhead":
        asyncio.run(compare_with_without_profiling())
    else:
        asyncio.run(test_real_website())
