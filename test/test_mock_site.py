import asyncio
import json
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from test.test_mock_values import run_mock_server
from crawler import IntelligentCrawler
from crawler.crawler_engine import CrawlConfig


async def test_with_mock_server():
    """ëª¨ì˜ ì„œë²„ë¡œ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ (topic ì§€ì •)"""
    print("=" * 70)
    print("Mock Server Crawl Test - WITH TOPIC")
    print("=" * 70)

    # ëª¨ì˜ ì„œë²„ ì‹œì‘
    print("\n[1/5] Starting mock server...")
    server_runner = await run_mock_server(8888)

    try:
        # ì„¤ì •
        config = CrawlConfig(
            max_pages=6,  # 6ê°œ í˜ì´ì§€
            max_depth=2,  # ê¹Šì´ 2
            request_delay=0.1,  # ë¡œì»¬ì´ë¯€ë¡œ ë¹ ë¥´ê²Œ
            respect_robots_txt=True,
        )

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        print("\n[2/5] Initializing components...")
        crawler = IntelligentCrawler(config, enable_profiling=True)
        print("  âœ“ All components initialized")

        # í¬ë¡¤ë§ ì‹œì‘
        seed_url = "http://localhost:8888/"
        topic = "artificial intelligence machine learning"

        print("\n[3/5] Starting crawl...")
        print(f"  Seed URL: {seed_url}")
        print(f"  Topic: {topic}")
        print(f"  Max pages: {config.max_pages}")
        print(f"  Max depth: {config.max_depth}")
        print()

        results = await crawler.crawl(
            seed_urls=[seed_url],
            topic=topic,
        )

        print("\n[4/5] Crawl completed!")

        # ê²°ê³¼ ë¶„ì„
        stats = results["statistics"]
        print("\n  ğŸ“Š Statistics:")
        print(f"    Pages crawled: {stats['num_pages']}")
        print(f"    Links found: {stats['num_links']}")
        print(f"    Internal links: {stats.get('internal_links', 0)}")
        print(f"    External links: {stats.get('external_links', 0)}")
        print(f"    Avg relevance: {stats.get('avg_topic_relevance', 0):.3f}")
        print(f"    Graph density: {stats.get('density', 0):.3f}")

        # í˜ì´ì§€ë³„ ê´€ë ¨ë„
        print("\n  ğŸ“„ Pages by relevance:")
        for i, page in enumerate(results["pages"], 1):
            relevance_bar = "â–ˆ" * int(page["topic_relevance"] * 20)
            print(f"    {i}. [{page['topic_relevance']:.3f}] {relevance_bar}")
            print(f"       {page['title']}")
            print(f"       Depth: {page['depth']}, PageRank: {page['pagerank']:.4f}")

        # ë§í¬ ê´€ê³„
        print("\n  ğŸ”— Link relationships:")
        for i, link in enumerate(results["links"][:10], 1):
            print(f"    {i}. [{link['relevance']:.3f}] {link['anchor_text'][:40]}")
            print(
                f"       {link['source'].split('/')[-1]} â†’ {link['target'].split('/')[-1]}"
            )

        # ê²€ì¦
        print("\n[5/5] Validation:")

        # ê´€ë ¨ í˜ì´ì§€ê°€ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ì•˜ëŠ”ì§€ í™•ì¸
        ai_pages = [
            p
            for p in results["pages"]
            if any(
                word in p["title"].lower()
                for word in ["ai", "machine", "learning", "neural", "deep"]
            )
        ]
        unrelated = [
            p
            for p in results["pages"]
            if "cooking" in p["title"].lower() or "recipe" in p["title"].lower()
        ]

        if ai_pages:
            avg_ai_relevance = sum(p["topic_relevance"] for p in ai_pages) / len(
                ai_pages
            )
            print(f"  âœ“ AI-related pages average relevance: {avg_ai_relevance:.3f}")

        if unrelated:
            avg_unrelated_relevance = sum(
                p["topic_relevance"] for p in unrelated
            ) / len(unrelated)
            print(
                f"  âœ“ Unrelated pages average relevance: {avg_unrelated_relevance:.3f}"
            )

            if avg_unrelated_relevance < 0.5:
                print("  âœ“ Good! Unrelated content has low relevance")
            else:
                print("  âš  Warning: Unrelated content has high relevance")

        # ê²°ê³¼ ì €ì¥
        output_data = {
            "topic": topic,
            "seed_urls": [seed_url],
            "statistics": stats,
            "pages": results["pages"],
            "links": results["links"],
        }

        # Performance ë°ì´í„° ì¶”ê°€
        if "performance" in results:
            output_data["performance"] = results["performance"]
            print("\n  âš¡ Performance Summary:")
            perf = results["performance"]
            print(f"    Total time: {perf['total_time']:.2f}s")
            print(f"    Memory usage: {perf['current_memory_mb']:.2f}MB")
            print(f"    CPU usage: {perf['cpu_percent']:.1f}%")

        with open("mock_test_results.json", "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        print("\n  ğŸ’¾ Results saved to: mock_test_results.json")

        print("\n" + "=" * 70)
        print("âœ“ Mock server test completed successfully!")
        print("=" * 70)

    except Exception as e:
        print(f"\nâœ— Error during test: {e}")
        import traceback

        traceback.print_exc()

    finally:
        # ì„œë²„ ì¢…ë£Œ
        print("\nğŸ›‘ Shutting down mock server...")
        await server_runner.cleanup()


if __name__ == "__main__":
    asyncio.run(test_with_mock_server())
