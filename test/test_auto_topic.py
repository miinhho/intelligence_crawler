import asyncio
import json
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from test.test_mock_values import run_mock_server
from crawler import IntelligentCrawler
from crawler.crawler_engine import CrawlConfig


async def test_auto_topic_discovery():
    """ëª¨ì˜ ì„œë²„ë¡œ ìë™ ì£¼ì œ íƒìƒ‰ í…ŒìŠ¤íŠ¸ (topic ì—†ìŒ)"""

    print("\n" + "=" * 70)
    print("Mock Server Crawl Test - AUTO TOPIC DISCOVERY")
    print("=" * 70)

    # ëª¨ì˜ ì„œë²„ ì‹œì‘
    print("\n[1/5] Starting mock server...")
    server_runner = await run_mock_server(8889)

    try:
        # ì„¤ì •
        config = CrawlConfig(
            max_pages=8,  # ë” ë§ì€ í˜ì´ì§€ë¡œ ì£¼ì œ íƒìƒ‰
            max_depth=2,
            request_delay=0.1,
            respect_robots_txt=True,
        )

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        print("\n[2/5] Initializing components...")
        crawler = IntelligentCrawler(config, enable_profiling=True)
        print("  âœ“ All components initialized")

        # í¬ë¡¤ë§ ì‹œì‘ (topic ì—†ìŒ)
        seed_url = "http://localhost:8889/"

        print("\n[3/5] Starting crawl with auto topic discovery...")
        print(f"  Seed URL: {seed_url}")
        print("  Topic: (auto-discover)")
        print(f"  Max pages: {config.max_pages}")
        print(f"  Max depth: {config.max_depth}")
        print()

        results = await crawler.crawl(
            seed_urls=[seed_url],
            topic=None,  # No topic - auto-discover
        )

        print("\n[4/5] Crawl completed!")

        # ìë™ íƒìƒ‰ëœ ì£¼ì œ í‘œì‹œ
        if results.get("auto_discovered_topic"):
            print("\n  ğŸ” Auto-discovered topic:")
            print(f"     {results['auto_discovered_topic']}")
        else:
            print("\n  âš  Topic was not discovered (not enough pages)")

        # ê²°ê³¼ ë¶„ì„
        stats = results["statistics"]
        print("\n  ğŸ“Š Statistics:")
        print(f"    Pages crawled: {stats['num_pages']}")
        print(f"    Links found: {stats['num_links']}")
        print(f"    Internal links: {stats.get('internal_links', 0)}")
        print(f"    External links: {stats.get('external_links', 0)}")
        print(f"    Avg relevance: {stats.get('avg_topic_relevance', 0):.3f}")
        print(f"    Graph density: {stats.get('density', 0):.3f}")

        # í˜ì´ì§€ë³„ ê´€ë ¨ë„
        print("\n  ğŸ“„ Pages by relevance:")
        for i, page in enumerate(results["pages"], 1):
            relevance_bar = "â–ˆ" * int(page["topic_relevance"] * 20)
            print(f"    {i}. [{page['topic_relevance']:.3f}] {relevance_bar}")
            print(f"       {page['title']}")
            print(f"       Depth: {page['depth']}, PageRank: {page['pagerank']:.4f}")

        # ê²€ì¦
        print("\n[5/5] Validation:")

        # AI ê´€ë ¨ í˜ì´ì§€ ì²´í¬
        ai_pages = [
            p
            for p in results["pages"]
            if any(
                word in p["title"].lower()
                for word in ["ai", "machine", "learning", "neural", "deep"]
            )
        ]
        unrelated = [
            p
            for p in results["pages"]
            if "cooking" in p["title"].lower() or "recipe" in p["title"].lower()
        ]

        if ai_pages:
            avg_ai_relevance = sum(p["topic_relevance"] for p in ai_pages) / len(
                ai_pages
            )
            print(f"  âœ“ AI-related pages average relevance: {avg_ai_relevance:.3f}")

        if unrelated:
            avg_unrelated_relevance = sum(
                p["topic_relevance"] for p in unrelated
            ) / len(unrelated)
            print(
                f"  âœ“ Unrelated pages average relevance: {avg_unrelated_relevance:.3f}"
            )

            if avg_unrelated_relevance < 0.5:
                print("  âœ“ Good! Auto-discovery filtered out unrelated content")
            else:
                print("  âš  Warning: Unrelated content has high relevance")
        else:
            print("  âœ“ Great! No unrelated pages were crawled")

        # ê²°ê³¼ ì €ì¥
        output_data = {
            "topic": results.get("auto_discovered_topic", "auto-discovered"),
            "seed_urls": [seed_url],
            "statistics": stats,
            "pages": results["pages"],
            "links": results["links"],
        }

        if "performance" in results:
            output_data["performance"] = results["performance"]
            print("\n  âš¡ Performance Summary:")
            perf = results["performance"]
            print(f"    Total time: {perf['total_time']:.2f}s")
            print(f"    Memory usage: {perf['current_memory_mb']:.2f}MB")
            print(f"    CPU usage: {perf['cpu_percent']:.1f}%")

        with open("mock_test_auto_topic_results.json", "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        print("\n  ğŸ’¾ Results saved to: mock_test_auto_topic_results.json")

        print("\n" + "=" * 70)
        print("âœ“ Auto topic discovery test completed successfully!")
        print("=" * 70)

    except Exception as e:
        print(f"\nâœ— Error during test: {e}")
        import traceback

        traceback.print_exc()

    finally:
        # ì„œë²„ ì¢…ë£Œ
        print("\nğŸ›‘ Shutting down mock server...")
        await server_runner.cleanup()


if __name__ == "__main__":
    asyncio.run(test_auto_topic_discovery())
